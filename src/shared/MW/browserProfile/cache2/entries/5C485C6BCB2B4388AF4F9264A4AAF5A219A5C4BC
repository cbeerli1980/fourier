[{"data":{"person":{"learner":{"programs":[{"programId":"c4cc7072aa4e0d5b5b16e723bdbce38d","liveSeminarDisplayAddToCalendar":true,"__typename":"Program"}],"course":{"title":"SchweserNotes - Book 1","rules":{"allowBookmarks":true,"showGlossaryNavIcon":false,"__typename":"CourseRules","showGlossaryToolTip":false},"__typename":"Course","nodes":[{"nodeId":4740659,"activity":{"activityId":18943,"name":"Module 4.2 Study - Supervised Learning Algorithms","isComplete":false,"nextActivity":{"activityId":18944,"name":"Module 4.2 Quiz","durationMinutes":12,"markedForReview":false,"isAccessible":true,"expectedDate":null,"__typename":"Activity"},"__typename":"Activity"},"quizRules":null,"lineage":[{"nodeId":4740625,"__typename":"Node"},{"nodeId":4740654,"__typename":"Node"},{"nodeId":4740655,"__typename":"Node"}],"title":"Module 4.2: Supervised Learning Algorithms","userStats":{"isBookmarked":false,"bookmarkNote":null,"__typename":"NodeUserStats","isComplete":false},"__typename":"Node","rules":{"hideVideos":false,"__typename":"NodeRules","timeAccruedUponCompletion":false,"canShowMarkComplete":true},"content":{"contentText":"<!--Merge in subtopics.-->\n<h3>LOS 4.c: Describe supervised machine learning algorithmsâ€”including penalized regression, support vector machine, k-nearest neighbor, classification and regression tree, ensemble learning, and random forestâ€”and determine the problems for which they are best suited.</h3><p outputclass=\"CFAI_ref\">CFA<sup>Â®</sup> Program Curriculum, Volume 1, page 264</p><p>We will now describe some of the common supervised ML algorithms and their applications:</p><ol><li><p><strong>Penalized regressions.</strong> Penalized regression models reduce the problem of overfitting by imposing a penalty based on the number of features used by the model. The penalty value increases with the number of independent variables (features) used. Imposing such a penalty can exclude features that are not meaningfully contributing to out-of-sample prediction accuracy (i.e., it makes the model more parsimonious). Penalized regression models seek to minimize the sum of square errors (SSE) <em>as well as</em> a penalty value.</p><p><strong>Least absolute shrinkage and selection operator (LASSO).</strong> This is a popular penalized regression model. In addition to minimizing SSE, LASSO minimizes the sum of the absolute values of the slope coefficients. In such a framework, there is a tradeoff between reducing the SSE (by increasing the number of features) and the penalty imposed on the inclusion of more features. Through optimization, LASSO automatically eliminates the least predictive features. A penalty term, Î» (lambda), is the hyperparameter that determines the balance between overfitting the model and keeping it parsimonious.</p><p>A related method to reduce statistical variability in a high dimension data estimation problem is <strong>regularization</strong>. Regularization forces the beta coefficients of nonperforming features toward zero.</p><p>Investment analysts use LASSO to build parsimonious models. Regularization can be applied to nonlinear models, such as the estimation of a stable covariance matrix that can be used for mean-variance optimization.</p><div style=\"margin-left:30px; margin-right:50px;\"><p><strong>Professor's Note</strong></p><p>In everyday usage, <em>parsimonious</em> means stingy or penny-pinching. In the world of statistics, a parsimonious model is one that accomplishes the required level of explanation using as few predictor variables as possible.</p></div></li><li><p><strong>Support vector machine (SVM).</strong> SVM is a linear classification algorithm that separates the data into one of two possible classifiers (e.g., sell vs. buy). Given <em>n</em> features, an <em>n</em>-dimensional hyperplane divides a sample into one of the two possible classifications. SVM maximizes the probability of making a correct prediction by determining the boundary that is farthest away from all the observations. This boundary comprises a discriminant boundary as well as margins on the side of the boundary. The margins are determined by the support vectors, observations that are closest to the boundary. Misclassified observations in the training data are handled via <strong>soft margin classification</strong>. This adaptation optimizes the tradeoff between a wider margin and classification error. We should note that a more complex, nonlinear model can be used for classification as opposed to SVM to reduce classification error, but this requires more features and may result in overfitting.</p><p>Applications of SVM in investment management include classifying debt issuers into likely-to-default versus not-likely-to-default issuers, stocks-to-short versus not-to-short, and even classifying text (from news articles or company press releases) as positive or negative.</p></li><li><p><strong>K-nearest neighbor (KNN).</strong> More commonly used in classification (but sometimes in regression), this technique is used to classify an observation based on <em>nearness</em> to the observations in the training sample. The researcher specifies the value of <em>k</em>, the hyperparameter, triggering the algorithm to look for the <em>k</em> observations in the sample that are closest to the new observation that is being classified. The specification of <em>k</em> is important because if it is too small, it will result in a high error rate, and if it is too large, it will dilute the result by averaging across too many outcomes. Also, if <em>k</em> is even, there may be ties, with no clear winner. KNN is a powerful, nonparametric model, but it requires a specification of what it means to be <em>near</em>. Analysts need to have a clear understanding of the data and the underlying business to be able to specify the distance metric that needs to be optimized. Another issue with KNN is the specification of feature set; inclusion of irrelevant or correlated features can skew the results.</p><p>Investment applications of KNN include predicting bankruptcy, assigning a bond to a ratings class, predicting stock prices, and creating customized indices.</p></li><li><p><strong>Classification and regression trees (CART).</strong> Classification trees are appropriate when the target variable is categorical, and are typically used when the target is binary (e.g., an IPO will be successful vs. not successful). Logit models, discussed in a previous reading, are also used when the target is binary, but are ill-suited when there are significant nonlinear relationships among variables. In such cases, classification trees may be a viable alternative. Regression trees are appropriate when the target is continuous.</p><p>Classification trees assign observations to one of two possible classifications at each node. At the top of the tree, the top feature (i.e., the one most important in explaining the target) is selected, and a cutoff value <em>c</em> is estimated. Observations with feature values greater than <em>c</em> are assigned to one classification, and the remainder are assigned to the other classification. The resulting classes are then evaluated based on a second feature, and again divided into one of two classes. Every successive classification should result in a lower estimation error than the nodes that preceded it. The tree stops when the error cannot be reduced further, resulting in a terminal node as shown in Â <strong>Classification Tree Example</strong>.</p><p><div id=\"fig_1_cfaL3_topic_00224_7\"><p><em>Classification Tree Example</em></p><p product=\"digital\"><img src=\"https://static.kaplanlearn.com/assets/b4/5d/CFA_L2B1_image1.jpg\" alt=\"\"/></p></div></p><p>It should be noted that a feature may reappear in lower nodes of a tree with a different cutoff value if it helps in classification. The features and cutoff values are learned by the algorithm based on labeled training data.</p><p>To avoid overfitting, regularization criteria such as maximum tree depth, maximum number of decision nodes, and so on are specified by the researcher. Alternatively, sections of tree with minimal explanatory power are <strong>pruned</strong>.</p><p>CART is popular because it provides a visual explanation of the prediction process, compared to other algorithms that are often described as black boxes due to their opacity.</p><p>Investment applications of CART include detecting fraudulent financial statements and selecting stocks and bonds.</p></li><li><p><strong>Ensemble and Random Forest.</strong> Ensemble learning is the technique of combining predictions from multiple models rather than a single model. The ensemble method results in a lower average error rate because the different models cancel out noise. Two kinds of ensemble methods are used: aggregation of heterogeneous learners and aggregation of homogenous learners.</p><p>Under aggregation of heterogeneous learners, different algorithms are combined together via a <strong>voting classifier</strong>. The different algorithms each get a vote, and then we go with whichever answer gets the most votes. Ideally, the models selected will have sufficient diversity in approach, resulting in a greater level of confidence in the predictions.</p><p>Under aggregation of homogenous learners, the same algorithm is used, but on different training data. The different training data samples (used by the same model) can be derived by <strong>bootstrap aggregating</strong> or <strong>bagging</strong>. The process relies on generating random samples (bags) with replacement from the initial training sample.</p><p><strong>Random forest</strong> is a variant of classification trees whereby a large number of classification trees are trained using data bagged from the same data set. A randomly selected subset of features is used in creating each tree, and each tree is slightly different from the others. The process of using multiple classification trees to determine the final classification is akin to the practice of crowdsourcing. Because each tree only uses a subset of features, random forests can mitigate the problem of overfitting. Using random forests can increase the signal-to-noise ratio because errors across different trees tend to cancel each other out. A drawback of random forests is that the transparency of CART is lost, and we are back to the black-box category of algorithms.</p><p>Investment applications of random forest include factor-based asset allocation, and prediction models for the success of an IPO.</p></li></ol><div id=\"VideoPlayerContainer\" role=\"region\" aria-label=\"Video Player region\" aria-live=\"polite\"><div id=\"video_player_5792407846125568\"><div id=\"player_6085306118001\" style=\"margin-top: 10px;\"><param name=\"allowScriptAccess\" value=\"always\" /></div></div></div><br/>","__typename":"Content"},"video":{"title":"Module 4.2: Supervised Learning Algorithms","ucmsId":"5792407846125568","videoId":"6085306118001","enrollmentDetail":{"enrollmentDetailId":56872514,"__typename":"EnrollmentDetail"},"channel":{"channelId":56872514,"__typename":"VideoChannel"},"playerInstance":"jwplayer","accountId":"2974989255001","maxPosition":0,"playerId":"muVZWdr95","currentPosition":0,"mediaForceProgression":false,"mediaForceProgressionPercentage":0,"playerKey":"wJZUYvXlc/swMq+Y8AV5HwCIO9HjA2HYyguwqedGGC4=","playerScripts":["https://static.kaplanlearn.com/js/jwplayer/jwplayer.js","https://static.kaplanlearn.com/js/jwplayer/plugins/thegovernor/thegovernor.js","https://static.kaplanlearn.com/js/ucms-player.js"],"type":"embedded","thumbnailUrl":"http://f1.media.brightcove.com/8/2974989255001/2974989255001_6085307239001_6085306118001-vs.jpg?pubId=2974989255001&videoId=6085306118001","isPending":false,"isLive":false,"startDate":null,"duration":673,"presenter":null,"details":null,"sources":[{"url":"https://kaplanudso.akamaized.net/2974989255001/2974989255001_6085307758001_6085306118001.mp4?pubId=2974989255001&videoId=6085306118001","label":"1280x720 @ 990kbps","rate":"990000","__typename":"VideoSource"},{"url":"https://kaplanudso.akamaized.net/2974989255001/2974989255001_6085311129001_6085306118001.mp4?pubId=2974989255001&videoId=6085306118001","label":"720x404 @ 726kbps","rate":"726000","__typename":"VideoSource"},{"url":"https://kaplanudso.akamaized.net/2974989255001/2974989255001_6085308515001_6085306118001.mp4?pubId=2974989255001&videoId=6085306118001","label":"640x360 @ 596kbps","rate":"596000","__typename":"VideoSource"}],"tracks":[{"default":false,"url":"https://ucms.kaplan.com/caption/agxzfmthcGxhbnVjbXNyMAsSBlJlY29yZBiAgMDgoIWlCgwLEhBCYXNlSG9zdGVkUmVjb3JkGICAwID21foJDA","label":"English","kind":"captions","__typename":"VideoTrack"}],"playerSkinData":{"fullscreenButton":"https://static.kaplanlearn.com/js/jwplayer/skins/assets/fullscreenButton.png","skin":"https://static.kaplanlearn.com/js/jwplayer/skins/six_flash.xml","__typename":"PlayerSkinData"},"__typename":"Video"},"nextNodeWithContent":{"title":"Module 4.3: Unsupervised Learning Algorithms and Other Models","nodeId":4740660,"userStats":{"isAccessible":true,"__typename":"NodeUserStats"},"__typename":"Node"},"previousNodeWithContent":{"title":"Module 4.1: Types of Learning and Overfitting Problems","nodeId":4740658,"userStats":{"isAccessible":true,"__typename":"NodeUserStats"},"__typename":"Node"}}],"userStats":{"lastViewedNode":{"nodeId":4740658,"userStats":{"isAccessible":true,"__typename":"NodeUserStats"},"__typename":"Node"},"__typename":"CourseUserStats"}},"__typename":"Learner"},"__typename":"Person"}}}]wÄ6Ø›ð      aøOéaøOéCèXÿaøOé   _    O^partitionKey=%28https%2Ckaplanlearn.com%29,a,~1643659275,:https://gql.kaplanlearn.com/graphql necko:classified 1 strongly-framed 1 security-info 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 request-method POST auth Bearer response-head HTTP/2 200 OK
date: Mon, 31 Jan 2022 21:08:57 GMT
content-type: application/json
content-length: 13556
x-powered-by: Express
access-control-allow-origin: *
server-timing: dtRpid;desc="1776138170"
timing-allow-origin: *
X-Firefox-Spdy: h2
 original-response-headers date: Mon, 31 Jan 2022 21:08:57 GMT
content-type: application/json
content-length: 13556
x-powered-by: Express
access-control-allow-origin: *
server-timing: dtRpid;desc="1776138170"
set-cookie: dtCookie=v_4_srv_2_sn_19CD56AFE785543040DE728E0F4F470D_perc_100000_ol_0_mul_1; Path=/; Domain=.kaplan.com; secure
timing-allow-origin: *
X-Firefox-Spdy: h2
 ctid 1 net-response-time-onstart 388 net-response-time-onstop 389   4ô