[{"data":{"person":{"learner":{"programs":[{"programId":"c4cc7072aa4e0d5b5b16e723bdbce38d","liveSeminarDisplayAddToCalendar":true,"__typename":"Program"}],"course":{"title":"SchweserNotes - Book 1","rules":{"allowBookmarks":true,"showGlossaryNavIcon":false,"__typename":"CourseRules","showGlossaryToolTip":false},"__typename":"Course","nodes":[{"nodeId":4740667,"activity":{"activityId":18952,"name":"Module 5.3 Study - Model Training and Evaluation","isComplete":false,"nextActivity":{"activityId":18953,"name":"Module 5.3 Quiz","durationMinutes":18,"markedForReview":false,"isAccessible":true,"expectedDate":null,"__typename":"Activity"},"__typename":"Activity"},"quizRules":null,"lineage":[{"nodeId":4740625,"__typename":"Node"},{"nodeId":4740654,"__typename":"Node"},{"nodeId":4740662,"__typename":"Node"}],"title":"Module 5.3: Model Training and Evaluation","userStats":{"isBookmarked":false,"bookmarkNote":null,"__typename":"NodeUserStats","isComplete":false},"__typename":"Node","rules":{"hideVideos":false,"__typename":"NodeRules","timeAccruedUponCompletion":false,"canShowMarkComplete":true},"content":{"contentText":"<!--Merge in subtopics.-->\n<h3>LOS 5.d: Describe objectives, steps, and techniques in model training.</h3><p outputclass=\"CFAI_ref\">CFA<sup>Â®</sup> Program Curriculum, Volume 1, page 359</p><p>Before model training, it is important to define the objective(s) of data analysis, identify useful data points, and conceptualize the model. Model conceptualization is the iterative planning phase that lays out the process to be followed. This process gets tweaked until the desired results are achieved. It is important that ML engineers work with domain experts so as to identify data characteristics and relationships (e.g., the relation between inflation and exchange rates).</p><p>Once the unstructured data has been processed and codified in a structured form such as a data matrix, model training is similar to that of structured data. ML seeks to identify patterns in the data set via a set of rules. Model fitting describes how well the model generalizes to new data (i.e., how the model performs out of sample).</p><p>Model fitting errors can be caused by:</p><ul><li><em>Size of the training sample</em>. Small data sets do not provide adequate training and can lead to an underfit model that does not recognize important patterns.</li><li><em>Number of features</em>. Fewer features can also lead to an underfitting problem; the small number of features may not carry enough information to identify patterns in the training sample. On the other hand, data sets with a large number of features can lead to overfitting due to fewer degrees of freedom. Overfit models do not generalize well in the validation sample. The feature selection step discussed earlier is important in mitigating the overfitting and underfitting problems. FE, when properly done, tends to reduce the underfitting problem.</li></ul><div style=\"margin-left:30px; margin-right:50px;\"><p><strong>Professor's Note</strong></p><p>Model fitting is discussed in detail in the topic review on machine learning.</p></div><p>The three tasks of model training are as follows:</p><ol><li><p><strong>Method selection</strong> is the art and science of choosing the appropriate ML method (i.e., algorithm) given the objectives and data characteristics. Method selection is based on the following factors:</p><ul><li><em>Supervised or unsupervised learning</em>. Supervised learning is used when the training data contains the <strong>ground truth</strong> or the known outcome (i.e., the target variable). In such cases, available methods include regression, ensemble trees, <strong>support vector machines (SVMs)</strong>, and <strong>neural networks (NN)</strong>. Unsupervised learning occurs when there is no target variable. Unsupervised learning methods include clustering, dimension reduction, and anomaly detection.</li><li><em>Type of data</em>. For numerical data (e.g., predicting earnings) we may use classification and regression tree (CART) methods. For text data, we can use <strong>generalized linear models (GLMs)</strong> and SVMs. For image data, neural networks and deep learning methods can be employed.</li><li><em>Size of data</em>. Large data sets with many observations and features can be handled with SVMs. Neural networks work better with a large number of observations, but few features.</li></ul><div style=\"margin-left:30px; margin-right:50px;\"><p><strong>Professor's Note</strong></p><p>These methods and their applications are discussed in detail in the topic review on machine learning.</p></div><p>Once a method is selected, the researcher has to specify appropriate hyperparameters (e.g., the number of hidden layers in a neural network). For mixed data sets (containing numerical and textual data), multiple methods are often used. Sometimes, the output of one method (e.g., classification of financial news text for a company as positive or negative) may be used as an input to another model. Sometimes, multiple models are employed, and a weighted average of the forecasts from those models is used.</p><p>For supervised learning, before model training begins, the data set is divided into three parts. The larger part (â‰ˆ 60%) is used for model training. A second part (â‰ˆ 20%) is used for validation and model tuning. The last part (â‰ˆ 20%) is the test set, and is used to check the out-of-sample performance of the model. Due to the absence of labeled training data, no splitting of the data set is needed for unsupervised learning.</p><p>For a model to be able to discriminate well, it should be provided with a wide variety of the training data. <strong>Class imbalance</strong> occurs when one class has a large number of observations relative to other classes. For example, in a model for predicting bond default, if the data set has a large number of high-grade bonds (i.e., those that would be less likely to default), then the model would be more likely to predict nondefault for a new observation. The training data set should have a variety of high- and low-grade bonds so as to have enough diversity to make correct predictions. One way to overcome class imbalance is to undersample the overrepresented class and oversample the underrepresented class.</p></li><li><strong>Performance evaluation</strong> is the process of assessing model efficacy; various tools are used to quantify and critique model performance.</li><li><strong>Tuning</strong> is the process of implementing changes to improve model performance.</li></ol><p>These steps are recursively applied until a desired level of model performance is attained. We will next explore the performance evaluation and tuning steps in detail.</p><h3>LOS 5.g: Evaluate the fit of a machine learning algorithm.</h3><p outputclass=\"CFAI_ref\">CFA<sup>Â®</sup> Program Curriculum, Volume 1, page 385</p><h4>Techniques to Measure Model Performance</h4><p>In order to validate a model, we must measure its training performance or goodness of fit. We will next consider a few methods to measure this performance. (These techniques are particularly suited to evaluating binary classification models.)</p><ol><li><p><strong>Error analysis.</strong> Errors in classification problems can be false positives (type I error) or false negatives (type II error). A <strong>confusion matrix</strong> shows the results of a classification problem, as in Â <strong>Classification of Defaulters</strong>.</p><div id=\"fig_1_cfaL3_topic_00224_27\"><p><em>Classification of Defaulters</em></p><p><table cellpadding=\"5\" frame=\"hsides\" border=\"1\" bordercolor=\"#000000\" rules=\"none\"><tr class=\"border_bottom\"><th bgcolor=\"#cccccc\" align=\"center\"/><th bgcolor=\"#cccccc\" align=\"left\">Actual: Default</th><th bgcolor=\"#cccccc\" align=\"left\">Actual: No Default</th></tr><tr><td><strong>Prediction: Default</strong></td><td>True positive (TP)</td><td>False positive (FP, type I)</td></tr><tr><td><strong>Prediction: No Default</strong></td><td>False negative (FN, type II)</td><td>True negative (TN)</td></tr></table></p></div><p>Metrics such as <strong>precision</strong> (the ratio of true positives to all predicted positives) and <strong>recall</strong> (the ratio of TPs to all actual positives) can be used. High precision is valued when the cost of a type I error is large, while high recall is valued when the cost of a type II error is large.</p><p style=\"margin-left:60px;\">precision (P) = TP / (TP + FP)</p><p style=\"margin-left:60px;\">recall (R) = TP / (TP + FN)</p><p>While FP and FN are both errors, they may not be equally important. The tradeoff between precision and recall is a business decision, and depends on the model application. For example, a lender may want to avoid lending to potential defaulters, and so will want to maximize recall. Together, model precision and recall determine model <strong>accuracy</strong>, which is the proportion of correct forecasts out of a total number of forecasts. The <strong>F1 score</strong> is the harmonic mean of precision and recall.</p><p style=\"margin-left:60px;\">accuracy = (TP + TN) / (TP + TN + FP + FN)</p><p style=\"margin-left:60px;\">F1 score = (2 Ã— P Ã— R) / (P + R)</p></li><li><p><strong>Receiver operating characteristic (ROC).</strong> Also used for classification problems, the ROC is a curve that plots the tradeoff between FPs and TPs. The true positive rate (TPR) is the same as recall, and is plotted along the Y-axis. The false positive rate (FPR) is the ratio of FPs to all actual negatives, and is plotted along the X-axis.</p><p style=\"margin-left:60px;\">TPR = TP / (TP + FN)</p><p style=\"margin-left:60px;\">FPR = FP / (FP + TN)</p><p>Â <strong>ROC Curves and AUC</strong> shows the performance of the three models used to predict defaults. The area under the curve (AUC) is a value from 0 to 1. The closer the value of AUC is to 1, the higher the predictive accuracy of the model. An AUC value of 0.50 (indicated by a straight line, for model 1) indicates that the model makes a random guess. The higher the convexity of the ROC curve, the higher its AUC.</p><div id=\"fig_2_cfaL3_topic_00224_27\"><p><em>ROC Curves and AUC</em></p><p product=\"digital\"><img src=\"https://static.kaplanlearn.com/assets/10/7b/CFA_R8_image2.jpg\" alt=\"\"/></p></div></li><li><strong>Root mean square error (RMSE).</strong> This is useful for data predictions that are continuous, such as regression models. The RMSE is a single metric summarizing the prediction error in a sample. <p style=\"margin-left:60px;\"><mathml><m:math xmlns:m=\"http://www.w3.org/1998/Math/MathML\"><m:mrow><m:mtext>RMSE</m:mtext><m:mo>=</m:mo><m:msqrt><m:mrow><m:mstyle mathsize=\"16pt\"><m:mfrac><m:mrow><m:mstyle displaystyle=\"true\"><m:munderover><m:mo>âˆ‘</m:mo><m:mrow><m:mtext>i</m:mtext><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mtext>n</m:mtext></m:munderover><m:mrow><m:msup><m:mrow><m:mrow><m:mo>(</m:mo><m:mrow><m:msub><m:mrow><m:mtext>predicted</m:mtext></m:mrow><m:mtext>i</m:mtext></m:msub><m:mo>âˆ’</m:mo><m:msub><m:mrow><m:mtext>actual</m:mtext></m:mrow><m:mtext>i</m:mtext></m:msub></m:mrow><m:mo>)</m:mo></m:mrow></m:mrow><m:mn>2</m:mn></m:msup></m:mrow></m:mstyle></m:mrow><m:mtext>n</m:mtext></m:mfrac></m:mstyle></m:mrow></m:msqrt></m:mrow></m:math></mathml></p></li></ol><div style=\"margin-left:30px; margin-right:50px;\"><p><strong>Example: Model evaluation</strong></p><p>Dave Kwah is evaluating a model that predicts whether a company is likely to have a dividend cut next year. The model uses a binary classification: cut versus not cut. In the test sample consisting of 78 observations, the model correctly classified 18 companies that had a dividend cut, as well as 46 companies that did not have a dividend cut. The model failed to identify three companies that actually had a dividend cut.</p><ol><li>Calculate the model's precision and recall.</li><li>Calculate the model's accuracy and F1 score.</li><li>Calculate the model's FPR.</li></ol><p><strong>Answer:</strong></p><p><table cellpadding=\"5\" frame=\"hsides\" border=\"1\" bordercolor=\"#000000\" rules=\"none\"><tr class=\"border_bottom\"><th bgcolor=\"#cccccc\" align=\"center\"/><th bgcolor=\"#cccccc\" align=\"left\">Actual: Cut</th><th bgcolor=\"#cccccc\" align=\"left\">Actual: Not Cut</th></tr><tr><td><strong>Prediction: Cut</strong></td><td>TP = 18</td><td>FP = 11</td></tr><tr><td><strong>Prediction: Not Cut</strong></td><td>FN = 3</td><td>TN = 46</td></tr></table></p><ol><li><p>Precision = TP / (TP + FP) = 18 / (18 + 11) = 0.62</p><p>Recall = TP / (TP + FN) = 18 / (18 + 3) = 0.86</p></li><li><p>Accuracy = (TP + TN) / (TP + TN + FP + FN)</p><p>= (18 + 46) / (18 + 3 + 11 + 46) = 64 / 78 = 0.82</p><p>F1 score = (2 Ã— P Ã— R) / (P + R) = (2 Ã— 0.62 Ã— 0.86) / (0.62 + 0.86) = 1.07 / 1.48 = 0.72</p></li><li>FPR = FP / (TN + FP) = 11 / (46 + 11) = 0.19</li></ol></div><h4>Model Tuning</h4><p>After model evaluation, the model needs to be revised until it reaches an acceptable performance level. <strong>Bias error</strong> is the prediction error in the training data resulting from underfit models. Bias errors occur from oversimplified models, which don't learn adequately from the training sample. <strong>Variance error</strong> is the prediction error in the validation sample resulting from overfitting models that do not generalize well. Overfitting is an issue with supervised ML that results when too many features are included in the training sample (i.e., the model is too complicated). It is necessary to find an optimum tradeoff between bias and variance errors, such that the model is neither underfitting nor overfitting.</p><p>A <strong>fitting curve</strong> is a plot of training error and cross-validation prediction error with varying model complexity (more complex = more features). An example of a fitting curve is shown in Â <strong>Fitting Curve</strong>.</p><div id=\"fig_3_cfaL3_topic_00224_27\"><p><em>Fitting Curve</em></p><p product=\"digital\"><img src=\"https://static.kaplanlearn.com/assets/5d/bf/CFA_R8_image3.jpg\" alt=\"\"/></p></div><p>As a model's complexity increases, it starts overfitting the training sample, and training error (i.e., bias error) declines. However, this decrease in bias error comes at the cost of increasing variance error. Regularization seeks to reduce model complexity by imposing a penalty on features that don't meaningfully contribute to the predictive power of the model. Optimal model complexity balances the tradeoff between bias and variance error.</p><p><strong>Parameters</strong> are estimated by the model (e.g., slope coefficients in a regression model) using an optimization technique on the training sample. <strong>Hyperparameters</strong> (e.g., the number of hidden layers in a neural network, or the <em>p</em>-threshold in logistic regression) are specified by ML engineers, and are not dependent on the training sample.</p><p>Tuning involves altering the hyperparameters until a desirable level of model performance is achieved. For each specification of hyperparameter(s), a confusion matrix is prepared based on the classification results, and accuracy and F1 scores are compiled. Rather than using a trial-and-error approach, especially if there are multiple hyperparameters in the model, one can use a <strong>grid search</strong>. A grid search is an automated process of selecting the best combination of hyperparameters.</p><p><strong>Ceiling analysis</strong> is an evaluation and tuning of each of the components in the entire model-building pipeline. It identifies the weak link in the process, which can be tuned to improve the performance of the model.</p><div id=\"VideoPlayerContainer\" role=\"region\" aria-label=\"Video Player region\" aria-live=\"polite\"><div id=\"video_player_5013095761051648\"><div id=\"player_6085309812001\" style=\"margin-top: 10px;\"><param name=\"allowScriptAccess\" value=\"always\" /></div></div></div><br/>","__typename":"Content"},"video":{"title":"Module 5.3: Model Training and Evaluation","ucmsId":"5013095761051648","videoId":"6085309812001","enrollmentDetail":{"enrollmentDetailId":56872514,"__typename":"EnrollmentDetail"},"channel":{"channelId":56872514,"__typename":"VideoChannel"},"playerInstance":"jwplayer","accountId":"2974989255001","maxPosition":0,"playerId":"muVZWdr95","currentPosition":0,"mediaForceProgression":false,"mediaForceProgressionPercentage":0,"playerKey":"wJZUYvXlc/swMq+Y8AV5HwCIO9HjA2HYyguwqedGGC4=","playerScripts":["https://static.kaplanlearn.com/js/jwplayer/jwplayer.js","https://static.kaplanlearn.com/js/jwplayer/plugins/thegovernor/thegovernor.js","https://static.kaplanlearn.com/js/ucms-player.js"],"type":"embedded","thumbnailUrl":"http://f1.media.brightcove.com/8/2974989255001/2974989255001_6085308281001_6085309812001-vs.jpg?pubId=2974989255001&videoId=6085309812001","isPending":false,"isLive":false,"startDate":null,"duration":1002,"presenter":null,"details":null,"sources":[{"url":"https://kaplanudso.akamaized.net/2974989255001/2974989255001_6085308789001_6085309812001.mp4?pubId=2974989255001&videoId=6085309812001","label":"1280x720 @ 992kbps","rate":"992000","__typename":"VideoSource"},{"url":"https://kaplanudso.akamaized.net/2974989255001/2974989255001_6085308784001_6085309812001.mp4?pubId=2974989255001&videoId=6085309812001","label":"720x404 @ 727kbps","rate":"727000","__typename":"VideoSource"},{"url":"https://kaplanudso.akamaized.net/2974989255001/2974989255001_6085308251001_6085309812001.mp4?pubId=2974989255001&videoId=6085309812001","label":"640x360 @ 596kbps","rate":"596000","__typename":"VideoSource"}],"tracks":[{"default":false,"url":"https://ucms.kaplan.com/caption/agxzfmthcGxhbnVjbXNyMAsSBlJlY29yZBiAgMCApezzCAwLEhBCYXNlSG9zdGVkUmVjb3JkGICAwKCTwtcLDA","label":"English","kind":"captions","__typename":"VideoTrack"}],"playerSkinData":{"fullscreenButton":"https://static.kaplanlearn.com/js/jwplayer/skins/assets/fullscreenButton.png","skin":"https://static.kaplanlearn.com/js/jwplayer/skins/six_flash.xml","__typename":"PlayerSkinData"},"__typename":"Video"},"nextNodeWithContent":{"title":"Reading 5: Key Concepts by LOS","nodeId":4740668,"userStats":{"isAccessible":true,"__typename":"NodeUserStats"},"__typename":"Node"},"previousNodeWithContent":{"title":"Module 5.2: Data Exploration","nodeId":4740666,"userStats":{"isAccessible":true,"__typename":"NodeUserStats"},"__typename":"Node"}}],"userStats":{"lastViewedNode":{"nodeId":4740666,"userStats":{"isAccessible":true,"__typename":"NodeUserStats"},"__typename":"Node"},"__typename":"CourseUserStats"}},"__typename":"Learner"},"__typename":"Person"}}}]ò¥Eª|      aøPaøPCèYaøP   _    O^partitionKey=%28https%2Ckaplanlearn.com%29,a,~1643659291,:https://gql.kaplanlearn.com/graphql necko:classified 1 strongly-framed 1 security-info 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 request-method POST auth Bearer response-head HTTP/2 200 OK
date: Mon, 31 Jan 2022 21:09:27 GMT
content-type: application/json
content-length: 18670
x-powered-by: Express
access-control-allow-origin: *
server-timing: dtRpid;desc="-1256201607"
timing-allow-origin: *
X-Firefox-Spdy: h2
 original-response-headers date: Mon, 31 Jan 2022 21:09:27 GMT
content-type: application/json
content-length: 18670
x-powered-by: Express
access-control-allow-origin: *
server-timing: dtRpid;desc="-1256201607"
set-cookie: dtCookie=v_4_srv_9_sn_828305DE7DEC2DF87D49FCDABD8AF330_perc_100000_ol_0_mul_1; Path=/; Domain=.kaplan.com; secure
timing-allow-origin: *
X-Firefox-Spdy: h2
 ctid 1 net-response-time-onstart 364 net-response-time-onstop 364   Hî